import numpy as np
import pandas as pd
from sklearn.model_selection import LeaveOneOut
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import os
import warnings

# Suppress warnings for small datasets
warnings.filterwarnings('ignore')

# Load data
data = pd.read_csv('/Users/simrankour/Desktop/papersec.csv')

print(f"Dataset shape: {data.shape}")
print(f"Dataset columns: {data.columns.tolist()}")
print(f"Dataset preview:")
print(data)

# Define topological indices and properties
topological_indices = ['W', 'WW', 'H', 'D', 'DH']
# Update with your actual 6 properties
properties = ['BP', 'E','FP','MR','P','MV']

# Check which properties exist
available_properties = [prop for prop in properties if prop in data.columns]
print(f"\nAvailable properties: {available_properties}")
print(f"Available topological indices: {[idx for idx in topological_indices if idx in data.columns]}")

# KNN hyperparameters for small datasets
k_values = [1, 2, 3, 4, 5]  # Limited to reasonable values for 10 samples
distance_metrics = ['euclidean', 'manhattan']
weight_options = ['uniform', 'distance']

# Test both scaled and unscaled data
scaling_options = [True, False]  # True for scaled, False for unscaled

# Storage for results
all_results = []
detailed_results = {}

# Create output directory
output_dir = '/Users/simrankour/Desktop/KNN_Results'
os.makedirs(output_dir, exist_ok=True)

print("\n" + "=" * 60)
print("KNN ANALYSIS FOR SMALL DATASET (10 SAMPLES)")
print("Using Leave-One-Out Cross-Validation")
print("Testing both SCALED and UNSCALED data")
print("=" * 60)

for prop_idx, prop in enumerate(available_properties, 1):
    print(f"\nAnalyzing Property {prop_idx}: {prop}")
    print("-" * 40)
    
    # Get clean data for this property
    available_indices = [idx for idx in topological_indices if idx in data.columns]
    prop_data = data[[prop] + available_indices].dropna()
    n_samples = len(prop_data)
    
    print(f"  Clean samples for {prop}: {n_samples}")
    
    if n_samples < 3:
        print(f"  Insufficient data for {prop} (need at least 3 samples)")
        continue
    
    property_results = []
    
    for index in available_indices:
        print(f"  Testing topological index: {index}")
        
        X = prop_data[[index]].values
        y = prop_data[prop].values
        
        # Test both scaled and unscaled versions
        for use_scaling in scaling_options:
            scaling_label = "Scaled" if use_scaling else "Unscaled"
            print(f"    Testing {scaling_label} data...")
            
            best_loo_score = -np.inf
            best_params = None
            best_predictions = None
            
            # Limit k to reasonable values for small dataset
            max_k = min(max(k_values), n_samples - 1)
            valid_k_values = [k for k in k_values if k <= max_k]
            
            # Hyperparameter optimization using LOOCV
            for k in valid_k_values:
                for metric in distance_metrics:
                    for weights in weight_options:
                        # Skip distance weighting for k=1 (doesn't make sense)
                        if k == 1 and weights == 'distance':
                            continue
                        
                        # Leave-One-Out Cross-Validation
                        loo = LeaveOneOut()
                        loo_predictions = []
                        loo_actuals = []
                        
                        try:
                            for train_idx, test_idx in loo.split(X):
                                X_train, X_test = X[train_idx], X[test_idx]
                                y_train, y_test = y[train_idx], y[test_idx]
                                
                                # Apply scaling if requested
                                if use_scaling:
                                    scaler = StandardScaler()
                                    X_train_processed = scaler.fit_transform(X_train)
                                    X_test_processed = scaler.transform(X_test)
                                else:
                                    X_train_processed = X_train
                                    X_test_processed = X_test
                                
                                knn = KNeighborsRegressor(
                                    n_neighbors=k, 
                                    metric=metric, 
                                    weights=weights
                                )
                                knn.fit(X_train_processed, y_train)
                                
                                pred = knn.predict(X_test_processed)[0]
                                loo_predictions.append(pred)
                                loo_actuals.append(y_test[0])
                            
                            # Calculate LOOCV metrics
                            if len(loo_predictions) > 1:
                                loo_r2 = r2_score(loo_actuals, loo_predictions)
                                loo_rmse = np.sqrt(mean_squared_error(loo_actuals, loo_predictions))
                                loo_mae = mean_absolute_error(loo_actuals, loo_predictions)
                                
                                if loo_r2 > best_loo_score:
                                    best_loo_score = loo_r2
                                    best_params = {
                                        'k': k, 
                                        'metric': metric, 
                                        'weights': weights
                                    }
                                    best_predictions = loo_predictions.copy()
                                    best_rmse = loo_rmse
                                    best_mae = loo_mae
                            
                        except Exception as e:
                            print(f"      Error with k={k}, metric={metric}, weights={weights}: {str(e)[:50]}...")
                            continue
            
            # Store results for this index and scaling option
            if best_params is not None:
                # Train final model on all data for full predictions
                if use_scaling:
                    scaler_final = StandardScaler()
                    X_processed = scaler_final.fit_transform(X)
                else:
                    X_processed = X
                
                final_knn = KNeighborsRegressor(
                    n_neighbors=best_params['k'],
                    metric=best_params['metric'],
                    weights=best_params['weights']
                )
                final_knn.fit(X_processed, y)
                final_predictions = final_knn.predict(X_processed)
                
                # Calculate metrics on full data (for reference)
                full_r2 = r2_score(y, final_predictions)
                full_rmse = np.sqrt(mean_squared_error(y, final_predictions))
                full_mae = mean_absolute_error(y, final_predictions)
                
                index_result = {
                    'Property': prop,
                    'Topological_Index': index,
                    'Scaling': scaling_label,
                    'LOOCV_R2': best_loo_score,
                    'LOOCV_RMSE': best_rmse,
                    'LOOCV_MAE': best_mae,
                    'Full_Data_R2': full_r2,
                    'Full_Data_RMSE': full_rmse,
                    'Full_Data_MAE': full_mae,
                    'Best_K': best_params['k'],
                    'Best_Metric': best_params['metric'],
                    'Best_Weights': best_params['weights'],
                    'N_Samples': n_samples
                }
                property_results.append(index_result)
                
                print(f"      {scaling_label}:")
                print(f"        LOOCV - R²={best_loo_score:.4f}, RMSE={best_rmse:.4f}, MAE={best_mae:.4f}")
                print(f"        Full  - R²={full_r2:.4f}, RMSE={full_rmse:.4f}, MAE={full_mae:.4f}")
                print(f"        Best params: k={best_params['k']}, metric={best_params['metric']}, weights={best_params['weights']}")
                
                # Save individual predictions
                pred_df = pd.DataFrame({
                    'Sample_Index': range(len(y)),
                    f'{prop}_Actual': y,
                    f'{prop}_LOOCV_Predicted': best_predictions,
                    f'{prop}_Full_Predicted': final_predictions,
                    f'{prop}_LOOCV_Residuals': np.array(loo_actuals) - np.array(best_predictions),
                    f'{prop}_Full_Residuals': y - final_predictions,
                    f'{prop}_Scaling': [scaling_label] * len(y),
                    f'{prop}_K_Value': [best_params['k']] * len(y),
                    f'{prop}_Distance_Metric': [best_params['metric']] * len(y),
                    f'{prop}_Weights': [best_params['weights']] * len(y)
                })
                pred_filename = f'{output_dir}/KNN_{prop}_{index}_{scaling_label}_predictions.csv'
                pred_df.to_csv(pred_filename, index=False)
                
            else:
                print(f"      {scaling_label}: No valid model found")
    
    # Store detailed results for this property
    if property_results:
        detailed_results[prop] = property_results
        
        # Find best index for this property (based on LOOCV R²)
        best_result = max(property_results, key=lambda x: x['LOOCV_R2'])
        all_results.append(best_result)
        
        print(f"\n  BEST for {prop}: {best_result['Topological_Index']} ({best_result['Scaling']})")
        print(f"    LOOCV R²: {best_result['LOOCV_R2']:.4f}")
        print(f"    LOOCV RMSE: {best_result['LOOCV_RMSE']:.4f}")
        print(f"    LOOCV MAE: {best_result['LOOCV_MAE']:.4f}")
        print(f"    Best params: k={best_result['Best_K']}, {best_result['Best_Metric']}, {best_result['Best_Weights']}")
        
        # Save detailed results for this property
        prop_df = pd.DataFrame(property_results)
        prop_filename = f'{output_dir}/KNN_{prop}_all_indices_scaled_unscaled.csv'
        prop_df.to_csv(prop_filename, index=False)
        print(f"    All results saved to: {prop_filename}")
    
    else:
        print(f"  No valid results for {prop}")

# Final Summary
print("\n" + "=" * 60)
print("FINAL SUMMARY")
print("=" * 60)

if all_results:
    summary_df = pd.DataFrame(all_results)
    
    print("\nBest Index for Each Property (based on LOOCV):")
    print("-" * 70)
    for _, row in summary_df.iterrows():
        print(f"{row['Property']:>12}: {row['Topological_Index']:>3} ({row['Scaling']:>8}) "
              f"(R²={row['LOOCV_R2']:6.3f}, RMSE={row['LOOCV_RMSE']:6.3f}, "
              f"MAE={row['LOOCV_MAE']:6.3f}) [k={row['Best_K']}, {row['Best_Metric']}, {row['Best_Weights']}]")
    
    # Save summary
    summary_filename = f'{output_dir}/KNN_summary_best_per_property_with_scaling.csv'
    summary_df.to_csv(summary_filename, index=False)
    print(f"\nSummary saved to: {summary_filename}")
    
    # Overall statistics
    print(f"\nOverall Statistics Across All Properties:")
    print(f"  Mean LOOCV R²: {summary_df['LOOCV_R2'].mean():.4f}")
    print(f"  Mean LOOCV RMSE: {summary_df['LOOCV_RMSE'].mean():.4f}")
    print(f"  Mean LOOCV MAE: {summary_df['LOOCV_MAE'].mean():.4f}")
    print(f"  Properties analyzed: {len(summary_df)}")
    
    # Scaling preference analysis
    print(f"\nScaling Preference Analysis:")
    scaling_counts = summary_df['Scaling'].value_counts()
    for scaling_type, count in scaling_counts.items():
        print(f"  {scaling_type}: {count} properties ({count/len(summary_df)*100:.1f}%)")
    
    # K-value distribution analysis
    print(f"\nOptimal K-value Distribution:")
    k_counts = summary_df['Best_K'].value_counts().sort_index()
    for k, count in k_counts.items():
        print(f"  k={k}: {count} properties")
    
    # Distance metric distribution
    print(f"\nOptimal Distance Metric Distribution:")
    metric_counts = summary_df['Best_Metric'].value_counts()
    for metric, count in metric_counts.items():
        print(f"  {metric}: {count} properties")
    
    # Weight distribution
    print(f"\nOptimal Weight Distribution:")
    weight_counts = summary_df['Best_Weights'].value_counts()
    for weight, count in weight_counts.items():
        print(f"  {weight}: {count} properties")
    
    # Create comprehensive results file
    all_detailed = []
    for prop, results in detailed_results.items():
        all_detailed.extend(results)
    
    if all_detailed:
        comprehensive_df = pd.DataFrame(all_detailed)
        comprehensive_filename = f'{output_dir}/KNN_comprehensive_all_results_with_scaling.csv'
        comprehensive_df.to_csv(comprehensive_filename, index=False)
        print(f"  Comprehensive results: {comprehensive_filename}")
        
        # Analysis of scaling effects
        print(f"\nScaling Effects Analysis:")
        scaled_results = comprehensive_df[comprehensive_df['Scaling'] == 'Scaled']
        unscaled_results = comprehensive_df[comprehensive_df['Scaling'] == 'Unscaled']
        
        if len(scaled_results) > 0 and len(unscaled_results) > 0:
            print(f"  Scaled data - Mean R²: {scaled_results['LOOCV_R2'].mean():.4f}")
            print(f"  Unscaled data - Mean R²: {unscaled_results['LOOCV_R2'].mean():.4f}")
            print(f"  Scaled data - Mean RMSE: {scaled_results['LOOCV_RMSE'].mean():.4f}")
            print(f"  Unscaled data - Mean RMSE: {unscaled_results['LOOCV_RMSE'].mean():.4f}")
        
        # Analysis of best parameters across all combinations
        print(f"\nParameter Analysis Across All Property-Index Combinations:")
        print(f"  Most common k-values: {comprehensive_df['Best_K'].mode().tolist()}")
        print(f"  Most common distance metric: {comprehensive_df['Best_Metric'].mode().tolist()}")
        print(f"  Most common weighting: {comprehensive_df['Best_Weights'].mode().tolist()}")
        print(f"  Most common scaling: {comprehensive_df['Scaling'].mode().tolist()}")
        
        # Scaling vs K-value analysis
        print(f"\nScaling vs K-value Analysis:")
        scaling_k_crosstab = pd.crosstab(comprehensive_df['Scaling'], comprehensive_df['Best_K'])
        print(scaling_k_crosstab)
        
        # Scaling vs Distance metric analysis
        print(f"\nScaling vs Distance Metric Analysis:")
        scaling_metric_crosstab = pd.crosstab(comprehensive_df['Scaling'], comprehensive_df['Best_Metric'])
        print(scaling_metric_crosstab)

else:
    print("No valid results found!")
    print("\nPossible issues:")
    print("1. Property columns don't exist in your CSV")
    print("2. Too many missing values")
    print("3. Data quality issues")

print(f"\nAll files saved in: {output_dir}")
print("\nKNN Notes for 10-sample datasets:")
print("- Both scaled and unscaled data tested")
print("- Feature scaling applied using StandardScaler when requested")
print("- k-values limited to reasonable range (1-5)")
print("- LOOCV provides unbiased evaluation")
print("- Distance weighting may help with small k values")
print("- Use LOOCV metrics for model comparison")
print("- Scaling preference analysis included")
print("\nKNN Advantages:")
print("- Non-parametric (no assumptions about data distribution)")
print("- Simple and interpretable")
print("- Good for small datasets")
print("- Can capture local patterns")
print("- Works well with both scaled and unscaled data")
print("\nAnalysis complete!")